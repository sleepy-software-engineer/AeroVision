\begin{abstract}
    Vision Transformers (ViTs) excel in image recognition but require large datasets and complex architectures. We introduce TinyViT, a minimalist ViT that retains core components patch embedding and attention while drastically reducing scale. By simplifying token processing and prioritizing parameter efficiency, we test whether a tiny ViT can achieve good accuracy despite limited complexity. Experiments on standard benchmarks show TinyViT delivers acceptable results even with reduced data requirements. This work demonstrates that minimalist transformer architectures can learn meaningful representations, offering a pathway to simpler, more accessible models without sacrificing core functionality. 
\end{abstract}
