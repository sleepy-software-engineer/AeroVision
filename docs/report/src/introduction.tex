\section{Introduction}

Transformers, introduced by \cite{vaswani2017attention} for natural language processing (NLP), have become the dominant architecture for sequence modeling due to their scalability and self-attention mechanisms. Inspired by their success in NLP, \cite{alexey2020image} pioneered the Vision Transformer (ViT), demonstrating that transformers can achieve state of the art results in image recognition by treating images as sequences of patch tokens. By splitting an image into fixed-size patches, linearly embedding them, and processing the sequence with a standard transformer encoder, ViT outperformed Convolutional Neural Networks (CNNs) \cite{he2016deep} on large-scale datasets like ImageNet when pretrained on massive datasets (JFT-300M). However, ViTâ€™s strong performance comes at a cost: it requires extensive computational resources and large pretraining datasets, raising practical barriers for adoption in settings where such infrastructure is unavailable.

In this work, we aim to (1) introduce the foundational mechanics of Vision Transformers and (2) present TinyViT, a minimalist implementation designed to test the viability of ViTs in simplified settings. Unlike the original ViT, which emphasizes scaling to massive datasets, TinyViT reduces architectural complexity employing fewer transformer layers, smaller embedding dimensions while retaining the core principles of patch-based processing and self-attention. We evaluate TinyViT on widely adopted benchmarks like CIFAR-10 and CIFAR-100 \cite{krizhevsky2009learning}, and STL-10 \cite{coates2011analysis}.